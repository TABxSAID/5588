{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS 5588 \u2014 Week 1: Hands-On Lab\n",
        "## Mini-RAG Pipeline: Embeddings \u2192 Retrieval \u2192 Grounded Generation\n",
        "\n",
        "**Goals:**\n",
        "- Generate semantic embeddings using a Transformer-based encoder\n",
        "- Build a vector index for fast similarity search\n",
        "- Retrieve top-k relevant document chunks\n",
        "- Inject retrieved context into an LLM prompt for grounded generation\n",
        "\n",
        "**Workflow:** GitHub \u2192 Colab \u2192 Hugging Face \u2192 Vector Store (FAISS / Chroma) \u2192 LLM\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GenAI Systems Context (Mini-RAG)\n",
        "This lab implements a **mini Retrieval-Augmented Generation (RAG)** pipeline:\n",
        "- A **Transformer encoder** produces semantic embeddings\n",
        "- A **vector index (FAISS)** enables fast retrieval\n",
        "- Retrieved context is what a downstream **LLM** would use for grounded generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u2014 Environment Setup\n",
        "Install required libraries. This may take ~1 minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u2014 Load Dataset & Model from Hugging Face Hub\n",
        "We use a lightweight news dataset and a sentence embedding model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "dataset = load_dataset(\"ag_news\", split=\"train[:200]\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = dataset[\"text\"]\n",
        "print(f\"Loaded {len(texts)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u2014 Create Embeddings\n",
        "These vectors represent semantic meaning and enable retrieval before generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "print('Embedding shape:', embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 \u2014 Build a Vector Index (FAISS)\n",
        "This simulates the retrieval layer in RAG systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(np.array(embeddings))\n",
        "print('Index size:', index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 \u2014 Retrieval Function\n",
        "Search for documents related to a query.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def search(query, k=3):\n",
        "    q_emb = model.encode([query])\n",
        "    distances, indices = index.search(np.array(q_emb), k)\n",
        "    return [texts[i] for i in indices[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 \u2014 Try It!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "search(\"artificial intelligence in healthcare\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "In 1\u20132 sentences, explain how embeddings enable retrieval before generation in GenAI systems.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
